# **Human–AI Work Framework**

**From content generation to thinking infrastructure**

A framework that maps what humans can reliably do with AI, not what AI *is*, but where responsibility naturally sits in cognitive work.

## **The Core Principle**

**AI does cognitive operations. Humans own outcomes.**

The boundary isn't about capability. It's about accountability.

AI can perform increasingly sophisticated cognitive operations, but can't:

* Own intent (what is this for?)  
* Accept consequences (what happens if this is wrong?)  
* Access implicit context (what else matters here?)

These aren't limitations that better models will solve. They're structural features of the relationship,  humans, not AI, exist in organizations, have reputations, and face consequences.

## **The Human Role**

Four responsibilities that define your role:

**Intent** \- What are we trying to achieve? Why now? For whom?

**Framing** \- What's in scope? What constraints matter? What does "good" look like?

**Judgement** \- What signal matters? Which trade-offs are acceptable? What do we ignore?

**Accountability** \- What do I stand behind? What decisions am I making? What if this is wrong?

These can't be delegated because they require understanding unstated context, accepting irreversible consequences, and applying personal or organizational values.

## **The AI Capability Set**

Nine operations AI performs reliably enough to delegate:

**Expansion** \- Create more from little  
Drafts, idea lists, options, hypotheses  
*→ Human responsibility: Choose direction and relevance*

**Compression** \- Make less, keep signal  
Summaries, key points, themes, comparisons  
*→ Human responsibility: Decide what signal matters*

**Transformation** \- Same content, new form  
Rewrite, reformat, change structure or voice  
*→ Human responsibility: Decide tone, intent, and omission*

**Exploration** \- Surface what's missing  
Blind spots, alternatives, counterarguments, recent changes  
*→ Human responsibility: Choose which unknowns to pursue*

**Validation** \- Stress-test thinking  
Contradictions, weak claims, logical gaps, hidden assumptions  
*→ Human responsibility: Decide which risks are acceptable*

**Decomposition** \- Turn vague into doable  
Steps, dependencies, work blocks  
*→ Human responsibility: Decide sequencing and ownership*

**Simulation** \- Model scenarios and perspectives  
Counterfactuals, role-play stakeholders, stress-test outcomes  
*→ Human responsibility: Judge whether the model matches reality enough to inform decisions*

**Retrieval & recombination** \- Reuse what exists  
Pull relevant past work, combine sources, reapply old thinking  
*→ Human responsibility: Judge relevance vs inertia*

**Synthesis** \- Transfer structure across domains  
Analogies, pattern transfer, reframing for insight  
*→ Human responsibility: Judge whether analogy clarifies or misleads*

## **What Makes Work Delegable**

**Delegate to AI when:**

* The goal can be specified (even loosely)  
* The output is intermediate (you'll review it)  
* Being approximately wrong is acceptable  
* The context is in the prompt or retrievable

**Keep with humans:**

* Defining what success means  
* Accepting consequences  
* Reading context that isn't written down  
* Making trade-offs between competing goods

## **The Working Loop**

**End of day / End of loop:**

1. State intent  
2. Frame the problem  
3. Select 1–3 AI capabilities to apply  
4. Provide inputs  
5. Walk away

**Start of day / Start of loop:**

1. Triage output  
2. Discard noise  
3. Apply judgement  
4. Decide next action

**Diagnostics:**

* Editing heavily? Your framing was weak  
* Discarding everything? Your intent was unclear

## **Anti‑Patterns: Where Responsibility Breaks**

These are not mistakes in using AI tools. They are failures to retain human responsibility. For example: 

**Judgment Laundering:**   
*Treating AI output as an external authority rather than a chosen input.*  
If you wouldn’t defend the decision without the model, you don’t own it.

**Delegation Inversion**  
*Letting AI define success criteria instead of generating options within a human-defined frame.*  
When the model sets the goal, intent has already been abdicated.

**Context Collapse**  
*Acting on outputs that ignore social, temporal, or organizational realities that were never written down.*  
Correct answers can still be wrong decisions.

Anti‑patterns are signals, not rules. When you notice one, the fix is almost always to restate intent, reframe the problem, or reclaim judgment.

## **The Litmus Tests**

**Before delegating:**

* Is the output intermediate?  
* Is being slightly wrong acceptable?  
* Will I review and decide anyway?

**If yes, delegate.**

**Before accepting output:**

* Does this change my starting point?  
* Does it save cognitive energy?  
* Does it clarify a decision?

**If no, stop and redesign the handoff.**

## **Calibrating the Boundary**

**The framework is dynamic, not fixed.**

What's safely delegable today depends on:

* **Model capability** (changes with new releases)  
* **Your skill** (prompt design, output evaluation)  
* **Consequence severity** (synthesis for slide decks ≠ synthesis for strategy)  
* **Observed reliability** (track where AI fails for *your* work)

**Calibration practice:**

* Start conservative (use AI for low-stakes work)  
* Track failure modes (where does output mislead or waste time?)  
* Adjust delegation boundaries based on evidence  
* Revisit as models improve or stakes change

**Example:** Synthesis might be low-risk for internal presentations but high-risk for client strategy. That boundary shifts as you learn which analogies AI generates reliably vs. which sound clever but mislead.

**The principle stays constant** (humans own outcomes), but **where you draw the line evolves** with experience and context.

