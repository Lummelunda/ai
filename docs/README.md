### Why this framework 

This framework grew out of a practical frustration.

On one side, there is constant noise: claims of full automation, “magic” prompts, headlines about jobs disappearing. On the other, very real examples of people getting meaningful leverage from LLMs. I wanted to understand where that leverage actually comes from, and how to use it without outsourcing judgement by accident.

The starting point was not what AI is, but what it reliably does. I focused on outputs. What kinds of cognitive work can an LLM perform well enough that I would willingly review rather than produce them myself? From there, a small set of repeatable capabilities emerged.

Once that list existed, the human side became unavoidable. Every time AI output fell flat, the problem traced back to the same source: I hadn't been clear about intent, hadn't framed the problem right, or hadn't judged the output critically. The quality depends far less on clever prompts than on whether humans hold their ground. So the second step was to articulate the human responsibilities that never really move, regardless of how capable models become.

That shift produced the core insight of the framework: this is not about replacing work, but about defining a stable collaboration boundary. When roles are explicit, collaboration becomes predictable.

From there, the rest followed naturally. If you know what AI can do and what humans must retain, you can describe:

- what work is safely delegable
- what should never be delegated  
- how a work loop can hand off to AI and resume later with leverage    

The initial motivation was practical: what if you could hand off cognitive work at day's end and resume the next morning with more options, clearer thinking, or validated assumptions? That “night shift” mental model clarified the loop structure, but the same pattern works at any time scale.

The final elements (litmus tests, anti-patterns, and calibration) came from pressure-testing the framework in practice. They're not the core, but they're essential. They help you catch mistakes before they compound and adjust the boundary as models improve or consequences shift.

The principle underneath all of it stays simple and stable:

AI performs cognitive operations.

Humans own outcomes.

Everything in this framework is an attempt to make that boundary explicit, usable, and hard to accidentally cross.


(This framework emerged from working with LLMs (text and code generation), and examples reflect that origin. But the principles are modality-agnostic.)
