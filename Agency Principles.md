# **Agency Principles**

  

**From content generation to thinking infrastructure**

  

A framework that maps what humans can reliably do with AI, not what AI _is_, but where responsibility naturally sits in cognitive work.

---

## **The Core Principle**

  

**AI does cognitive operations. Humans own outcomes.**

  

The boundary isn’t about capability. It’s about accountability.

  

AI can perform increasingly sophisticated cognitive operations, but cannot:

- Own intent (what is this for?)
    
- Accept consequences (what happens if this is wrong?)
    
- Access implicit context (what else matters here?)
    

  

These are not limitations that better models will eventually solve. They are structural features of the relationship. Humans, not AI, exist within organisations, hold authority, and face consequences when decisions go wrong.

---

## **Structural Preconditions for Delegation**

  

AI capabilities are not uniformly reliable across all forms of input.

  

The same cognitive operation (for example evaluation or synthesis) may succeed or degrade depending on how information is structured, how constraints are expressed, and how roles of information are mixed.

  

Delegation therefore requires not only clear intent and framing, but appropriate information architecture.

  

When outputs degrade, the cause is often not incorrect judgement or poor prompting, but a mismatch between the task being delegated and the structure of the provided context.

  

Structure does not guarantee correctness.

It reduces ambiguity and shifts error from silent to visible.

  

Responsibility remains human, but reliability is shaped by structure.

  

For this reason, responsibility discipline and context architecture should be treated as complementary, not interchangeable.

---

## **The Human Role**

  

Four responsibilities that define your role:

  

**Intent** – What are we trying to achieve? Why now? For whom?

  

**Framing** – What’s in scope? What constraints matter? What does “good” look like?

  

**Judgement** – What signal matters? Which trade-offs are acceptable? What do we ignore?

  

**Accountability** – What do I stand behind? What decisions am I making? What if this is wrong?

  

These cannot be delegated because they require understanding unstated context, accepting irreversible consequences, and applying personal or organisational values.

---

## **The AI Capability Set**

  

Nine operations AI performs reliably enough to be useful when humans retain judgement:

  

**Expansion** – Create more from little

Drafts, idea lists, options, hypotheses

_→ Human responsibility: Choose direction and relevance_

  

**Compression** – Make less, keep signal

Summaries, key points, themes, comparisons

_→ Human responsibility: Decide what signal matters_

  

**Transformation** – Same content, new form

Rewrite, reformat, change structure or voice

_→ Human responsibility: Decide tone, intent, and omission_

  

**Exploration** – Surface what’s missing

Blind spots, alternatives, counterarguments, recent changes

_→ Human responsibility: Choose which unknowns to pursue_

  

**Validation** – Stress-test thinking

Contradictions, weak claims, logical gaps, hidden assumptions

_→ Human responsibility: Decide which risks are acceptable_

  

**Decomposition** – Turn vague into doable

Steps, dependencies, work blocks

_→ Human responsibility: Decide sequencing and ownership_

  

**Simulation** – Model scenarios and perspectives

Counterfactuals, role-play stakeholders, stress-test outcomes

_→ Human responsibility: Judge whether the model reflects reality closely enough to inform thinking_

  

**Retrieval & recombination** – Reuse what exists

Pull relevant past work, combine sources, reapply old thinking

_→ Human responsibility: Judge relevance versus inertia_

  

**Synthesis** – Transfer structure across domains

Analogies, pattern transfer, reframing for insight

_→ Human responsibility: Judge whether analogy clarifies or misleads_

  

These categories are not exhaustive or perfectly distinct. They function as a practical mental model, not a taxonomy.

---

## **What Makes Work Delegable**

  

**Delegate to AI when:**

- The goal can be specified, even loosely
    
- The output is intermediate
    
- Being approximately wrong is acceptable
    
- A human will review and decide
    

  

**Keep with humans:**

- Defining what success means
    
- Accepting consequences
    
- Interpreting context that is not written down
    
- Making trade-offs between competing values
    

  

Delegation is not about trusting the model.

It is about containing the cost of being wrong.

---

## **The Working Loop**

  

**End of loop:**

1. State intent
    
2. Frame the problem
    
3. Select 1–3 AI operations to apply
    
4. Provide inputs
    
5. Pause or hand off
    

  

**Start of loop:**

1. Triage output
    
2. Discard noise
    
3. Apply judgement
    
4. Decide next action
    

  

**Diagnostics:**

- Editing heavily often signals weak framing
    
- Discarding everything often signals unclear intent
    

  

These are feedback signals, not failures.

---

## **Anti-Patterns: Where Responsibility Breaks**

  

These are not mistakes in tool usage.

They are failures to retain human responsibility.

  

**Judgment laundering**

Treating AI output as an external authority rather than a chosen input.

If you wouldn’t defend the decision without the model, you don’t own it.

  

**Delegation inversion**

Letting AI define success criteria instead of generating options within a human-defined frame.

When the model sets the goal, intent has already been abdicated.

  

**Context collapse**

Acting on outputs that ignore social, temporal, or organisational realities that were never written down.

Correct answers can still produce wrong decisions.

  

Anti-patterns are signals, not rules.

When they appear, the fix is usually to restate intent, reframe the task, or reclaim judgement.

---

## **The Litmus Tests**

  

**Before delegating:**

- Is the output intermediate?
    
- Is being slightly wrong acceptable?
    
- Will I review and decide anyway?
    

  

If yes, delegation is appropriate.

  

**Before accepting output:**

- Does this change my starting point?
    
- Does it reduce cognitive load?
    
- Does it clarify a decision?
    

  

If not, redesign the handoff.

---

## **Calibrating the Boundary**

  

**The framework is dynamic, not fixed.**

  

What is safely delegable depends on:

- **Model capability** (varies by release and task type)
    
- **Your skill** (framing, evaluation, domain knowledge)
    
- **Consequence severity** (internal draft ≠ external decision)
    
- **Observed reliability** (how the model fails in _your_ work)
    

  

Calibration is empirical, not theoretical.

  

**Practical calibration:**

- Start with low-stakes tasks
    
- Observe how errors appear, not just whether they occur
    
- Prefer tasks where failure is visible over those where it is subtle
    
- Adjust delegation boundaries as patterns emerge
    

  

Some operations may remain unreliable regardless of structure.

In those cases, AI can still inform thinking, but judgement must remain active.

  

**The principle stays constant**: humans own outcomes.

**Where the line is drawn evolves** with experience, context, and consequence.
